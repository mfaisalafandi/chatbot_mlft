{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:25.566478Z",
     "iopub.status.busy": "2025-01-06T12:19:25.566178Z",
     "iopub.status.idle": "2025-01-06T12:19:39.678098Z",
     "shell.execute_reply": "2025-01-06T12:19:39.677448Z",
     "shell.execute_reply.started": "2025-01-06T12:19:25.566444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:39.679464Z",
     "iopub.status.busy": "2025-01-06T12:19:39.678870Z",
     "iopub.status.idle": "2025-01-06T12:19:41.400355Z",
     "shell.execute_reply": "2025-01-06T12:19:41.399594Z",
     "shell.execute_reply.started": "2025-01-06T12:19:39.679439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_excel('./dataset/alzhemer/translated_alzhemer.xlsx')\n",
    "\n",
    "train, _ = train_test_split(data, test_size=0.15,random_state=42,shuffle=True)\n",
    "validation, test = train_test_split(_, test_size=0.5,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:41.401772Z",
     "iopub.status.busy": "2025-01-06T12:19:41.401123Z",
     "iopub.status.idle": "2025-01-06T12:19:41.407052Z",
     "shell.execute_reply": "2025-01-06T12:19:41.406215Z",
     "shell.execute_reply.started": "2025-01-06T12:19:41.401746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15300\n",
      "1350\n",
      "1350\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(validation))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:41.409415Z",
     "iopub.status.busy": "2025-01-06T12:19:41.409160Z",
     "iopub.status.idle": "2025-01-06T12:19:41.572426Z",
     "shell.execute_reply": "2025-01-06T12:19:41.571678Z",
     "shell.execute_reply.started": "2025-01-06T12:19:41.409394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('./model/save_data/data_train.csv')\n",
    "validation.to_csv('./model/save_data/data_validation.csv')\n",
    "test.to_csv('./model/save_data/data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Menghapus karakter yang tidak diperlukan\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?/:;(){}\\[\\]\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "train['Questions'] = train['Questions'].apply(clean_text)\n",
    "train['Answers'] = train['Answers'].apply(clean_text)\n",
    "validation['Questions'] = validation['Questions'].apply(clean_text)\n",
    "validation['Answers'] = validation['Answers'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:41.573703Z",
     "iopub.status.busy": "2025-01-06T12:19:41.573508Z",
     "iopub.status.idle": "2025-01-06T12:19:41.691792Z",
     "shell.execute_reply": "2025-01-06T12:19:41.691036Z",
     "shell.execute_reply.started": "2025-01-06T12:19:41.573685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train['text'] = \"<bos> \" + train['Questions'] + \" <bot> \" + train['Answers'] + \" <eos>\"\n",
    "validation['text'] = \"<bos> \" + validation['Questions'] + \" <bot> \" + validation['Answers'] + \" <eos>\"\n",
    "test['text'] = \"<bos> \" + test['Questions'] + \" <bot> \" + test['Answers'] + \" <eos>\"\n",
    "\n",
    "train = train.dropna(subset=['text'])\n",
    "validation = validation.dropna(subset=['text'])\n",
    "test = validation.dropna(subset=['text'])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train[['text']])\n",
    "validation_dataset = Dataset.from_pandas(validation[['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:41.692832Z",
     "iopub.status.busy": "2025-01-06T12:19:41.692614Z",
     "iopub.status.idle": "2025-01-06T12:19:41.707260Z",
     "shell.execute_reply": "2025-01-06T12:19:41.706377Z",
     "shell.execute_reply.started": "2025-01-06T12:19:41.692814Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panjang maksimum pada dataset train: 401.817908496732\n",
      "Panjang maksimum pada dataset validation: 399.8103703703704\n"
     ]
    }
   ],
   "source": [
    "train['text_length'] = train['text'].str.len()\n",
    "validation['text_length'] = validation['text'].str.len()\n",
    "\n",
    "print(\"Panjang maksimum pada dataset train:\", train['text_length'].mean())\n",
    "print(\"Panjang maksimum pada dataset validation:\", validation['text_length'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:41.708387Z",
     "iopub.status.busy": "2025-01-06T12:19:41.708098Z",
     "iopub.status.idle": "2025-01-06T12:19:47.656140Z",
     "shell.execute_reply": "2025-01-06T12:19:47.655414Z",
     "shell.execute_reply.started": "2025-01-06T12:19:41.708359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50261, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'cahya/gpt2-small-indonesian-522M'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
    "                                \"bos_token\": \"<bos>\",\n",
    "                                \"eos_token\": \"<eos>\"})\n",
    "tokenizer.add_tokens([\"<bot>\"])\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:47.657250Z",
     "iopub.status.busy": "2025-01-06T12:19:47.656946Z",
     "iopub.status.idle": "2025-01-06T12:19:58.670355Z",
     "shell.execute_reply": "2025-01-06T12:19:58.669465Z",
     "shell.execute_reply.started": "2025-01-06T12:19:47.657208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e62e64f01604f9c9ece8431be3a893a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa9c953ff9b43a29db371925feb8a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_labels(example):\n",
    "    tokens = tokenizer(\n",
    "        example['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=400\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tokens['input_ids'],\n",
    "        'attention_mask': tokens['attention_mask'],\n",
    "    }\n",
    "\n",
    "tokenized_datasets_train = train_dataset.map(add_labels, batched = True)\n",
    "tokenized_datasets_val = validation_dataset.map(add_labels, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:58.671597Z",
     "iopub.status.busy": "2025-01-06T12:19:58.671267Z",
     "iopub.status.idle": "2025-01-06T12:19:58.675832Z",
     "shell.execute_reply": "2025-01-06T12:19:58.675001Z",
     "shell.execute_reply.started": "2025-01-06T12:19:58.671572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "data_collator = load_data_collator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:58.677167Z",
     "iopub.status.busy": "2025-01-06T12:19:58.676844Z",
     "iopub.status.idle": "2025-01-06T12:19:58.787854Z",
     "shell.execute_reply": "2025-01-06T12:19:58.787114Z",
     "shell.execute_reply.started": "2025-01-06T12:19:58.677135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='/kaggle/working/mental_health_gpt_1_f',\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        eval_steps=500,\n",
    "        save_total_limit=2,\n",
    "        fp16=True,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        save_steps=500,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.001,\n",
    "        report_to=[],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:58.788852Z",
     "iopub.status.busy": "2025-01-06T12:19:58.788626Z",
     "iopub.status.idle": "2025-01-06T12:19:59.109521Z",
     "shell.execute_reply": "2025-01-06T12:19:59.108718Z",
     "shell.execute_reply.started": "2025-01-06T12:19:58.788833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_datasets_train,\n",
    "        eval_dataset=tokenized_datasets_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:19:59.110544Z",
     "iopub.status.busy": "2025-01-06T12:19:59.110278Z",
     "iopub.status.idle": "2025-01-06T14:58:37.945506Z",
     "shell.execute_reply": "2025-01-06T14:58:37.944628Z",
     "shell.execute_reply.started": "2025-01-06T12:19:59.110521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T14:58:37.948110Z",
     "iopub.status.busy": "2025-01-06T14:58:37.947863Z",
     "iopub.status.idle": "2025-01-06T14:58:39.207781Z",
     "shell.execute_reply": "2025-01-06T14:58:39.206942Z",
     "shell.execute_reply.started": "2025-01-06T14:58:37.948089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('./model/chatbot_gpt_2')\n",
    "tokenizer.save_pretrained('./model/chatbot_gpt_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T15:58:29.761690Z",
     "iopub.status.busy": "2025-01-06T15:58:29.761365Z",
     "iopub.status.idle": "2025-01-06T15:58:30.190796Z",
     "shell.execute_reply": "2025-01-06T15:58:30.190088Z",
     "shell.execute_reply.started": "2025-01-06T15:58:29.761665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagaimana pola makan memengaruhi kesehatan pembuluh darah pada pasien Alzheimer?\n",
      "Chatbot: Pola makan yang sehat dapat mengurangi risiko pembuluh darah, yang terkait dengan meningkatnya risiko penyakit jantung dan penyakit Alzheimer.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a response\n",
    "def generate_response(user_input):\n",
    "    # Format the input with special tokens\n",
    "    input_text = f\"<bos> {user_input} <bot>\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        do_sample=True,\n",
    "        max_length=512,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"<bot>\" in response:\n",
    "        response = response.split(\"<bot>\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "user_input = \"Bagaimana pola makan memengaruhi kesehatan pembuluh darah pada pasien Alzheimer?\"\n",
    "print(user_input)\n",
    "response = generate_response(user_input)\n",
    "print(f\"Chatbot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50261, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50261, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_path = './model/chatbot_gpt_2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.bos_token = \"<bos>\"\n",
    "tokenizer.eos_token = \"<eos>\"\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apa hubungan antara penyakit Alzheimer dan hilangnya minat dalam beraktivitas?\n",
      "Chatbot: Penyakit Alzheimer dapat memengaruhi jalur kognisi, jalur saraf yang penting untuk mengirimkan energi dan aktivitas di otak. Disregulasi dalam kognisi ini, yang umum terjadi pada tahap awal penyakit, termasuk kesulitan dalam memperhatikan dan memahami hubungan spasial antara neuron dan jalur saraf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faisal\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\faisal\\.cache\\huggingface\\hub\\models--cahya--gpt2-small-indonesian-522M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "def generate_response(user_input):\n",
    "    input_text = f\"<bos> {user_input} <bot>\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        do_sample=True,\n",
    "        max_length=512,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"<bot>\" in response:\n",
    "        response = response.split(\"<bot>\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "user_input = \"Apa hubungan antara penyakit Alzheimer dan hilangnya minat dalam beraktivitas?\"\n",
    "print(user_input)\n",
    "response = generate_response(user_input)\n",
    "print(f\"Chatbot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Bisakah yoga dan meditasi membantu mengurangi risiko Alzheimer setelah cedera kepala?\n",
      "Bot: Ya, yoga dan praktik meditasi dapat membantu mengurangi stres, meningkatkan kesejahteraan emosional, dan meningkatkan fungsi kognitif, sehingga berpotensi mengurangi risiko Alzheimer setelah cedera kepala.\n",
      "You: Bagaimana tekanan darah tinggi memengaruhi produksi faktor pertumbuhan di otak, dan dapatkah perubahan kadar faktor pertumbuhan memengaruhi risiko Alzheimer?\n",
      "Bot: Tekanan darah tinggi dapat memengaruhi produksi faktor pertumbuhan di otak, yang berpotensi memengaruhi risiko Alzheimer. Kontrol tekanan darah sangat penting untuk mempertahankan kadar faktor pertumbuhan yang optimal dan mendukung kesehatan kognitif.\n",
      "Chatbot: Sampai jumpa!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Sampai jumpa!\")\n",
    "        break\n",
    "    response = generate_response(user_input)\n",
    "    print(f\"You: {user_input}\")\n",
    "    print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Questions  \\\n",
      "0  Bagaimana perkembangan penyakit Alzheimer meme...   \n",
      "1  Dapatkah penelitian terapi gen menjelaskan dam...   \n",
      "2  Tradisi atau ritual apa yang menurut Anda berm...   \n",
      "3  Apakah ada cara untuk mengurangi risiko penyak...   \n",
      "4  Bagaimana penyakit Alzheimer diobati, dan apak...   \n",
      "\n",
      "                                             Answers  \n",
      "0  Seiring perkembangan penyakit Alzheimer, pende...  \n",
      "1  Penelitian terapi gen bertujuan untuk menjelas...  \n",
      "2  Berjalan-jalan santai di taman terdekat setiap...  \n",
      "3  Meskipun risikonya tidak dapat sepenuhnya dihi...  \n",
      "4  Meskipun tidak ada obatnya, latihan fisik, akt...  \n",
      "1350\n"
     ]
    }
   ],
   "source": [
    "# Dataset diubah menjadi format Excel secara manual karena terdapat masalah saat pembacaan format CSV\n",
    "test = pd.read_excel('./model/save_data/dup_test.xlsx')\n",
    "print(test.head())\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['generate'] = test['Questions'].apply(generate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan hasil generate response ke dalam file Excel agar dapat dievaluasi dengan mudah sewaktu-waktu\n",
    "output_path = './model/save_data/response/response.xlsx'\n",
    "test.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rata-rata Metrik Evaluasi:\n",
      "BLEU: 0.6315\n",
      "ROUGE-1: 0.5919\n",
      "ROUGE-L: 0.5410\n",
      "METEOR: 0.5157\n",
      "Perplexity: 90.0284\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "\n",
    "# Fungsi untuk menghitung perplexity\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings['input_ids'].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss.item()\n",
    "\n",
    "    perplexity = np.exp(loss)\n",
    "    return perplexity\n",
    "\n",
    "# Fungsi untuk menghitung metrik evaluasi\n",
    "def evaluate_metrics(reference, hypothesis):\n",
    "    smooth_fn = SmoothingFunction().method2\n",
    "    bleu_score = sentence_bleu([reference], hypothesis, smoothing_function=smooth_fn)\n",
    "\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = rouge.score(reference, hypothesis)\n",
    "\n",
    "    meteor_score = single_meteor_score(reference.split(), hypothesis.split())\n",
    "\n",
    "    return bleu_score, rouge_scores, meteor_score\n",
    "\n",
    "data_file = './model/save_data/response/response.xlsx'\n",
    "data = pd.read_excel(data_file)\n",
    "\n",
    "if 'Answers' in data.columns and 'generate' in data.columns:\n",
    "    results = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        question = row['Questions']\n",
    "        reference = row['Answers']\n",
    "        hypothesis = row['generate']\n",
    "\n",
    "        # Hitung metrik evaluasi\n",
    "        bleu, rouge, meteor = evaluate_metrics(reference, hypothesis)\n",
    "\n",
    "        # Hitung perplexity\n",
    "        perplexity = calculate_perplexity(model, tokenizer, hypothesis)\n",
    "\n",
    "        results.append({\n",
    "            'Questions': question,\n",
    "            'Reference': reference,\n",
    "            'Hypothesis': hypothesis,\n",
    "            'BLEU': bleu,\n",
    "            'ROUGE-1': rouge['rouge1'].fmeasure,\n",
    "            'ROUGE-L': rouge['rougeL'].fmeasure,\n",
    "            'METEOR': meteor,\n",
    "            'Perplexity': perplexity,\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"Rata-rata Metrik Evaluasi:\")\n",
    "    print(f\"BLEU: {results_df['BLEU'].mean():.4f}\")\n",
    "    print(f\"ROUGE-1: {results_df['ROUGE-1'].mean():.4f}\")\n",
    "    print(f\"ROUGE-L: {results_df['ROUGE-L'].mean():.4f}\")\n",
    "    print(f\"METEOR: {results_df['METEOR'].mean():.4f}\")\n",
    "    print(f\"Perplexity: {results_df['Perplexity'].mean():.4f}\")\n",
    "else:\n",
    "    print(\"Kolom 'Answers' atau 'generate' tidak ditemukan dalam data.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6418114,
     "sourceId": 10385432,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
